{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kzsCrtzAuD8"
      },
      "source": [
        "# **Link Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOzr1GgwAz3E"
      },
      "source": [
        "https://drive.google.com/drive/folders/1YxW7r-Xj2il-Njkzo8Ed7BRsUqmPKMeE?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKu2aU6at1US"
      },
      "source": [
        "# **Scraping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JGcakG1tVZc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import selenium\n",
        "import time\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.common.exceptions import StaleElementReferenceException \n",
        "from selenium import webdriver\n",
        "\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XwrumZhw50v"
      },
      "source": [
        "## **Initiate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE-oeOLPw-D5"
      },
      "outputs": [],
      "source": [
        "s=Service(\"C:\\SAINS DATA\\chromedriver.exe\")\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_experimental_option(\"detach\", True)\n",
        "options.add_argument('disable_infobars')\n",
        "# options.headless = True\n",
        "\n",
        "driver = webdriver.Chrome(service=s, options=options)\n",
        "url=\"https://www.youtube.com/results?search_query=python+101\"\n",
        "driver.get(url)\n",
        "driver.maximize_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcE-TwECw_6T"
      },
      "source": [
        "## **Scroll**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Q0nWIZxCYd"
      },
      "outputs": [],
      "source": [
        "SCROLL_PAUSE_TIME = 1.5\n",
        "\n",
        "# Get scroll height\n",
        "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    # Scroll down to bottom\n",
        "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "\n",
        "    # Wait to load page\n",
        "    time.sleep(SCROLL_PAUSE_TIME)\n",
        "\n",
        "    # Calculate new scroll height and compare with last scroll height\n",
        "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height\n",
        "\n",
        "# driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.CONTROL + Keys.HOME)\n",
        "# channel_vid = driver.find_elements(By.CSS_SELECTOR, 'a[id = \"video-title\"]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8dtiUwIxEWN"
      },
      "source": [
        "## **Get Video Links**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CJQhKHRxH4i"
      },
      "outputs": [],
      "source": [
        "parents = driver.find_elements(By.CSS_SELECTOR, 'ytd-video-renderer[class = \"style-scope ytd-item-section-renderer\"]')\n",
        "yt_link = []\n",
        "yt_views = []\n",
        "\n",
        "for parent in parents:\n",
        "    yt_link_elem = parent.find_element(By.CSS_SELECTOR, 'a[id = \"thumbnail\"]')\n",
        "    href = yt_link_elem.get_attribute('href')\n",
        "    if 'shorts' not in href:\n",
        "        yt_link.append(href)\n",
        "    \n",
        "        yt_views_elem = parent.find_element(By.CSS_SELECTOR, 'span[class = \"inline-metadata-item style-scope ytd-video-meta-block\"]')\n",
        "        view_text = yt_views_elem.text\n",
        "        yt_views.append(view_text)\n",
        "        \n",
        "    else : \n",
        "        print('membuang shorts')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2evZO9S-xKu8"
      },
      "source": [
        "## **Add to Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqk6PfqDxOAi"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'Link': yt_link,\n",
        "                'Views': yt_views\n",
        "                  })\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RutixKJjxQZB"
      },
      "outputs": [],
      "source": [
        "print(len(yt_link))\n",
        "print(len(yt_views))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usTkskiJxQ8N"
      },
      "source": [
        "## **Save to CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxe6dcPpxUUZ"
      },
      "outputs": [],
      "source": [
        "df.to_csv('youtube_link.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKbqrZjDxWoy"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv('youtube_link.csv')\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPd3hpnexYB9"
      },
      "source": [
        "## **Get Date and Detail Views**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1HYUEdRxbdt"
      },
      "outputs": [],
      "source": [
        "detail_views = []\n",
        "uploaded = []\n",
        "for i in yt_link:\n",
        "    driver.get(i)\n",
        "    time.sleep(1.5)\n",
        "    \n",
        "    expand = driver.find_element(By.ID, 'expand')\n",
        "    expand.click()\n",
        "    time.sleep(0.1)\n",
        "    \n",
        "    tgl = driver.find_element(By.XPATH, '//*[@id=\"info\"]/span[3]')\n",
        "    uploaded.append(tgl.text)\n",
        "    \n",
        "    penonton = driver.find_element(By.XPATH, '//*[@id=\"info\"]/span[1]')\n",
        "    detail_views.append(penonton.text)\n",
        "    \n",
        "    if len(detail_views)%10 == 0 and len(uploaded)%10 == 0:\n",
        "        print(len(detail_views), 'data telah terscrapping dari total', len(df), 'data')\n",
        "     \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLKjVkzsxeaD"
      },
      "source": [
        "## **Add to Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv9mgF-_xii6"
      },
      "outputs": [],
      "source": [
        "df2['Upload Date'] = uploaded\n",
        "df2['Detail Views'] = detail_views\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSorkGpnxkPk"
      },
      "source": [
        "## **Save to CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH6qLlq2xn79"
      },
      "outputs": [],
      "source": [
        "df2.to_csv('youtube_link_detail.csv', index = False)\n",
        "print('saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N88zX-psxpcE"
      },
      "source": [
        "## **Get Thumbnail**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlgAopF6xsWn"
      },
      "outputs": [],
      "source": [
        "from pythumb import Thumbnail\n",
        "\n",
        "for i in yt_link:\n",
        "    try:\n",
        "        t = Thumbnail(i)\n",
        "        t.fetch()\n",
        "        t.save('C:\\TSD\\youtube_thumbnail')\n",
        "    except FileExistsError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879KsE1NuHQs"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAMW41IFx31D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r57QUWUKx6j_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('youtube_link_detail.csv')\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ_MPvxEyDv3"
      },
      "source": [
        "## **Remove Duplicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h68kVXcx9MD"
      },
      "outputs": [],
      "source": [
        "df['Link'].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOy1uqYkx9t4"
      },
      "outputs": [],
      "source": [
        "df_fix = df.drop_duplicates(subset=['Link'], keep='first')\n",
        "df_fix['Link'].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90N0Yg7Xx-9z"
      },
      "outputs": [],
      "source": [
        "df_fix.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FIfqtk5yH1Y"
      },
      "source": [
        "## **Detail Views in Integer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc3sPlgNyAw4"
      },
      "outputs": [],
      "source": [
        "num_view = []\n",
        "for i in df_fix.loc[:, ('Detail Views')]:\n",
        "    num_view.append(i.split(' ')[0])\n",
        "\n",
        "df_fix.loc[:, ('Detail Views')] = num_view   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUyAJH8pyQYk"
      },
      "outputs": [],
      "source": [
        "df_fix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4g94k8AyRpf"
      },
      "source": [
        "## **Remove Comma**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojP5HGhfyQxK"
      },
      "outputs": [],
      "source": [
        "df_fix = df_fix.stack().str.replace(',','').unstack()\n",
        "df_fix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuNOw08nyWT3"
      },
      "source": [
        "## **Drop Videos with 0 Views**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVsny--NyZ6B"
      },
      "outputs": [],
      "source": [
        "df_fix.drop(df_fix.index[df_fix['Detail Views'] == 'No'], inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgwWO_GLybpu"
      },
      "source": [
        "## **Adjust Data Type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGIaltwJyf5w"
      },
      "outputs": [],
      "source": [
        "df_fix['Detail Views'] = df_fix['Detail Views'].apply(int)\n",
        "df_fix.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsjwHM2yyiBM"
      },
      "source": [
        "## **Remove Day and Month**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd0KWp3iylC6"
      },
      "outputs": [],
      "source": [
        "tahun = []\n",
        "for tanggal in df_fix['Upload Date']:\n",
        "    tanggal_split = tanggal.split(' ')[-1]\n",
        "    tahun.append(tanggal_split)\n",
        "\n",
        "df_fix['Upload Date'] = tahun\n",
        "df_fix.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vh4QaM2ynrS"
      },
      "source": [
        "## **Adjust Data Type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGxkH9zXyooe"
      },
      "outputs": [],
      "source": [
        "df_fix['Upload Date'] = df_fix['Upload Date'].apply(int)\n",
        "df_fix.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_ZnFZisyqn8"
      },
      "source": [
        "## **Filter Years**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMiE-cYTytH8"
      },
      "outputs": [],
      "source": [
        "df_fix.drop(df_fix.index[df_fix['Upload Date'] <= 2019], inplace = True)\n",
        "df_fix['Upload Date'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Acz8vbP0yvYs"
      },
      "outputs": [],
      "source": [
        "df_fix = df_fix.reset_index(drop=True)\n",
        "df_fix.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IAbzFsqywXe"
      },
      "source": [
        "## **Save to CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRJqNdxLy1Gc"
      },
      "outputs": [],
      "source": [
        "df_fix.to_csv('data bersih_2019-2022.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ6ZL3FruL2_"
      },
      "source": [
        "# **EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUgemBpXy3OS"
      },
      "outputs": [],
      "source": [
        "df_fix['Detail Views'].describe().apply(lambda x: format(x, '2f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkBgcVyVy5DQ"
      },
      "outputs": [],
      "source": [
        "df_sort_asc = df_fix.sort_values(by = 'Detail Views')[:150].reset_index(drop=True)\n",
        "df_sort_asc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-BdoqVRy6hC"
      },
      "outputs": [],
      "source": [
        "#100 - 24999\n",
        "# len(df_fix.loc[(df_fix['Detail Views'] >= 100) & (df_fix['Detail Views'] < 25000), :].sort_values(by = 'Detail Views'))\n",
        "df_fix.loc[(df_fix['Detail Views'] >= 100) & (df_fix['Detail Views'] < 25000), :].sort_values(by = 'Detail Views')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8JHRtTYy8JW"
      },
      "outputs": [],
      "source": [
        "#25000 - 249999\n",
        "len(df_fix.loc[(df_fix['Detail Views'] >= 25000) & (df_fix['Detail Views'] < 250000), :].sort_values(by = 'Detail Views'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbZTACcNy9xX"
      },
      "outputs": [],
      "source": [
        "# >250000\n",
        "len(df_fix.loc[(df_fix['Detail Views'] >= 250000), :].sort_values(by = 'Detail Views'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8urYPKLcy_h_"
      },
      "outputs": [],
      "source": [
        "class_names = ['Rendah', 'Sedang', 'Tinggi']\n",
        "batas_bin = [100, 24999, 249999, 19000000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15nbHmTvzBLi"
      },
      "outputs": [],
      "source": [
        "df_fix.drop(df_fix.index[df_fix['Detail Views'] < 100], inplace = True)\n",
        "len(df_fix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-60fmw6zCwt"
      },
      "source": [
        "## **Binning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8obpfbJzIJt"
      },
      "outputs": [],
      "source": [
        "df_fix['Detail Views'] = pd.cut(df_fix['Detail Views'], batas_bin, labels=class_names)\n",
        "df_fix['Detail Views'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXPe1gDzzJif"
      },
      "source": [
        "## **Get Youtube Video Identifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ5KP4cezM3P"
      },
      "outputs": [],
      "source": [
        "df_yt_code = df_fix.loc[:, ('Link', 'Detail Views')]\n",
        "yt_code = []\n",
        "for i in df_yt_code['Link']:\n",
        "    yt_code.append(i.split('=')[1])\n",
        "\n",
        "df_yt_code['Link'] = yt_code\n",
        "df_yt_code.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqsker2dzPYr"
      },
      "outputs": [],
      "source": [
        "data_train = df_yt_code.sample(frac = 0.80, random_state=10)\n",
        "data_val = df_yt_code.drop(data_train.index)\n",
        "\n",
        "print(len(data_train) + len(data_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joMvzAQxzP0G"
      },
      "source": [
        "## **Labelling Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHoq3XuJ2h7g"
      },
      "outputs": [],
      "source": [
        "# path_tinggi_train = 'train/Tinggi/'\n",
        "# path_sedang_train = 'train/Sedang/'\n",
        "# path_rendah_train = 'train/Rendah/'\n",
        "\n",
        "# for index, row in data_train.iterrows():\n",
        "#     if row['Detail Views'] == 'Tinggi':\n",
        "#         shutil.move('youtube_thumbnail/'+ row['Link'] + '.jpg', path_tinggi_train)\n",
        "#     elif row['Detail Views'] == 'Sedang':\n",
        "#         shutil.move('youtube_thumbnail/'+ row['Link'] + '.jpg', path_sedang_train)\n",
        "#     elif row['Detail Views'] == 'Rendah':\n",
        "#         shutil.move('youtube_thumbnail/'+ row['Link'] + '.jpg', path_rendah_train)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFiCzq1j2ihB"
      },
      "outputs": [],
      "source": [
        "# path_tinggi_val = 'val/Tinggi/'\n",
        "# path_sedang_val = 'val/Sedang/'\n",
        "# path_rendah_val = 'val/Rendah/'\n",
        "\n",
        "# for index, row in data_val.iterrows():\n",
        "#     if row['Detail Views'] == 'Tinggi':\n",
        "#         shutil.move('youtube_thumbnail/'+ row['Link'] + '.jpg', path_tinggi_val)\n",
        "#     elif row['Detail Views'] == 'Sedang':\n",
        "#         shutil.move('youtube_thumbnail/'+ row['Link'] + '.jpg', path_sedang_val)\n",
        "#     elif row['Detail Views'] == 'Rendah':\n",
        "#         shutil.move('youtube_thumbnail/'+ row['Link'] + '.jpg', path_rendah_val)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXa_HgnVzS79"
      },
      "outputs": [],
      "source": [
        "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
        "print(class_names_label)\n",
        "nb_classes = len(class_names)\n",
        "\n",
        "IMAGE_SIZE = (640, 360)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AgW3zuRzhIP"
      },
      "source": [
        "## **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UivKeaqFzkVH"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \n",
        "    datasets = ['train', 'val']\n",
        "    output = []\n",
        "    \n",
        "    # Iterate through training and test sets\n",
        "    for dataset in datasets:\n",
        "        \n",
        "        images = []\n",
        "        labels = []\n",
        "        \n",
        "        print(\"Loading {}\".format(dataset))\n",
        "        \n",
        "        # Iterate through each folder corresponding to a category\n",
        "        for folder in os.listdir(dataset):\n",
        "            label = class_names_label[folder]\n",
        "            \n",
        "            # Iterate through each image in our folder\n",
        "            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n",
        "                \n",
        "                # Get the path name of the image\n",
        "                img_path = os.path.join(os.path.join(dataset, folder), file)\n",
        "                \n",
        "                # Open and resize the img\n",
        "                image = cv2.imread(img_path)\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = cv2.resize(image, IMAGE_SIZE) \n",
        "                \n",
        "                # Append the image and its corresponding label to the output\n",
        "                images.append(image)\n",
        "                labels.append(label)\n",
        "                \n",
        "        images = np.array(images, dtype = 'uint8')\n",
        "        labels = np.array(labels, dtype = 'int32')   \n",
        "        \n",
        "        output.append((images, labels))\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzdfiYzaznTZ"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDVh08Q7zoeZ"
      },
      "outputs": [],
      "source": [
        "n_train = train_labels.shape[0]\n",
        "n_test = test_labels.shape[0]\n",
        "\n",
        "print (\"Number of training examples: {}\".format(n_train))\n",
        "print (\"Number of testing examples: {}\".format(n_test))\n",
        "print (\"Each image is of size: {}\".format(train_images.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tA0bN6Czo6Y"
      },
      "outputs": [],
      "source": [
        "train_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzaG-IQozqqE"
      },
      "outputs": [],
      "source": [
        "_, train_counts = np.unique(train_labels, return_counts=True)\n",
        "_, test_counts = np.unique(test_labels, return_counts=True)\n",
        "pd.DataFrame({'train': train_counts,\n",
        "                'test': test_counts} \n",
        "                ).plot.bar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGFobubpzsT9"
      },
      "outputs": [],
      "source": [
        "def display_examples(class_names, images, labels):\n",
        "    \"\"\"\n",
        "        Display 25 images from the images array with its corresponding labels\n",
        "    \"\"\"\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    fig.suptitle(\"Some examples of images of the dataset\", fontsize=16)\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "        plt.xlabel(class_names[labels[i]])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfCE5QgAzvKQ"
      },
      "outputs": [],
      "source": [
        "display_examples(class_names, train_images, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0svVxqd0zvm2"
      },
      "outputs": [],
      "source": [
        "def RGB2HEX(color):\n",
        "    return \"#{:02x}{:02x}{:02x}\".format(int(color[0]), int(color[1]), int(color[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QzIJW7hzxFO"
      },
      "outputs": [],
      "source": [
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv2.imread(os.path.join(folder,filename))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, IMAGE_SIZE) \n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return np.array(images, dtype='uint8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tQQ1q90zyjf"
      },
      "outputs": [],
      "source": [
        "all_images = np.concatenate((train_images, test_images), axis=0)\n",
        "\n",
        "rendah_images = np.concatenate((load_images_from_folder('train/Rendah/'), \n",
        "                                load_images_from_folder('val/Rendah/')), axis=0)\n",
        "\n",
        "sedang_images = np.concatenate((load_images_from_folder('train/Sedang/'), \n",
        "                                load_images_from_folder('val/Sedang/')), axis=0)\n",
        "\n",
        "tinggi_images = np.concatenate((load_images_from_folder('train/Tinggi/'), \n",
        "                                load_images_from_folder('val/Tinggi/')), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGqBWQSiz0Kx"
      },
      "outputs": [],
      "source": [
        "all_images_reshape = all_images.reshape(-1, all_images.shape[1]*all_images.shape[2]*all_images.shape[3])\n",
        "rendah_reshape = rendah_images.reshape(-1, rendah_images.shape[1]*rendah_images.shape[2]*rendah_images.shape[3])\n",
        "sedang_reshape = sedang_images.reshape(-1, sedang_images.shape[1]*sedang_images.shape[2]*sedang_images.shape[3])\n",
        "tinggi_reshape = tinggi_images.reshape(-1, tinggi_images.shape[1]*tinggi_images.shape[2]*tinggi_images.shape[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5N06hFOz1lm"
      },
      "source": [
        "## **Dominant Colors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkG57gukz5gR"
      },
      "source": [
        "### **All Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11ALHJqtz8fq"
      },
      "outputs": [],
      "source": [
        "number_of_colors = 5\n",
        "clf = KMeans(n_clusters = number_of_colors, random_state = 10)\n",
        "labels = clf.fit_predict(all_images_reshape)\n",
        "# labels_rendah = clf.fit_predict(rendah_reshape)\n",
        "# labels_sedang = clf.fit_predict(sedang_reshape)\n",
        "# labels_tinggi = clf.fit_predict(tinggi_reshape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioQBXvRvz-FE"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(labels)\n",
        "\n",
        "center_colors = clf.cluster_centers_\n",
        "# We get ordered colors by iterating through the keys\n",
        "ordered_colors = [center_colors[i] for i in counts.keys()]\n",
        "hex_colors = [RGB2HEX(ordered_colors[i]) for i in counts.keys()]\n",
        "print(hex_colors)\n",
        "rgb_colors = [ordered_colors[i] for i in counts.keys()]\n",
        "\n",
        "plt.figure(figsize = (8, 6))\n",
        "plt.pie(counts.values(), labels = hex_colors, colors = hex_colors, autopct='%.0f%%')\n",
        "plt.title('Most Used Colors of All Images')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1crswePg0BXB"
      },
      "source": [
        "### **Rendah**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnurR28y0D_i"
      },
      "outputs": [],
      "source": [
        "clf2 = KMeans(n_clusters = number_of_colors, random_state = 10)\n",
        "labels_rendah = clf2.fit_predict(rendah_reshape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFABbEHb0Fgz"
      },
      "outputs": [],
      "source": [
        "counts_rendah = Counter(labels_rendah)\n",
        "\n",
        "center_colors2 = clf2.cluster_centers_\n",
        "# We get ordered colors by iterating through the keys\n",
        "ordered_colors = [center_colors2[i] for i in counts_rendah.keys()]\n",
        "hex_colors = [RGB2HEX(ordered_colors[i]) for i in counts_rendah.keys()]\n",
        "rgb_colors = [ordered_colors[i] for i in counts_rendah.keys()]\n",
        "\n",
        "plt.figure(figsize = (8, 6))\n",
        "plt.pie(counts_rendah.values(), labels = hex_colors, colors = hex_colors, autopct='%.0f%%')\n",
        "plt.title('Most Used Colors of Images with Rendah Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpSW2I630KUO"
      },
      "source": [
        "### **Sedang**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYylobJl0MOV"
      },
      "outputs": [],
      "source": [
        "clf1 = KMeans(n_clusters = number_of_colors, random_state = 10)\n",
        "labels_sedang = clf1.fit_predict(sedang_reshape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52nGKM6Q0N6E"
      },
      "outputs": [],
      "source": [
        "counts_sedang = Counter(labels_sedang)\n",
        "\n",
        "center_colors1 = clf1.cluster_centers_\n",
        "# We get ordered colors by iterating through the keys\n",
        "ordered_colors = [center_colors1[i] for i in counts_sedang.keys()]\n",
        "hex_colors = [RGB2HEX(ordered_colors[i]) for i in counts_sedang.keys()]\n",
        "rgb_colors = [ordered_colors[i] for i in counts_sedang.keys()]\n",
        "\n",
        "plt.figure(figsize = (8, 6))\n",
        "plt.pie(counts_sedang.values(), labels = hex_colors, colors = hex_colors, autopct='%.0f%%')\n",
        "plt.title('Most Used Colors of Images with Sedang Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGeMQHvG0PSs"
      },
      "source": [
        "### **Tinggi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpBSwKMR0Rhj"
      },
      "outputs": [],
      "source": [
        "clf3 = KMeans(n_clusters = number_of_colors, random_state = 10)\n",
        "labels_tinggi = clf3.fit_predict(tinggi_reshape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuBBRG4E0T4Y"
      },
      "outputs": [],
      "source": [
        "counts_tinggi = Counter(labels_tinggi)\n",
        "\n",
        "center_colors3 = clf3.cluster_centers_\n",
        "# We get ordered colors by iterating through the keys\n",
        "ordered_colors = [center_colors3[i] for i in counts_tinggi.keys()]\n",
        "hex_colors = [RGB2HEX(ordered_colors[i]) for i in counts_tinggi.keys()]\n",
        "rgb_colors = [ordered_colors[i] for i in counts_tinggi.keys()]\n",
        "\n",
        "plt.figure(figsize = (8, 6))\n",
        "plt.pie(counts_tinggi.values(), labels = hex_colors, colors = hex_colors, autopct='%.0f%%')\n",
        "plt.title('Most Used Colors of Images with Tinggi Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yXpNlHguPqV"
      },
      "source": [
        "# **Analisis Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZKq0V7X0Xli"
      },
      "source": [
        "## **Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDhXUzJn0aTl"
      },
      "outputs": [],
      "source": [
        "train_images = train_images/255\n",
        "test_images = test_images/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxLMLzB-0cFv"
      },
      "source": [
        "## **PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rLuob9n0euJ"
      },
      "outputs": [],
      "source": [
        "X = train_images.reshape(-1, train_images.shape[1]*train_images.shape[2]*train_images.shape[3])\n",
        "print('ukuran data train (fitur):', X.shape)\n",
        "\n",
        "X_scaling = StandardScaler().fit_transform(X)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq_dU_-U0gQj"
      },
      "outputs": [],
      "source": [
        "pca = PCA().fit(X_scaling)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-yC877Z0iBA"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=200, random_state=10)\n",
        "X_pca = pca.fit_transform(X)\n",
        "print('dimensi data setelah PCA:', X_pca.shape)\n",
        "\n",
        "approximation = pca.inverse_transform(X_pca)\n",
        "plt.figure(figsize=(8,4))\n",
        "n = 200\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(X[n].reshape(train_images.shape[1], train_images.shape[2], train_images.shape[3]), cmap=plt.get_cmap('gray'))\n",
        "plt.xlabel(str(X.shape[1])+'components', fontsize = 14)\n",
        "plt.title('original image', fontsize = 20)\n",
        "plt.grid(False)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(approximation[n].reshape(train_images.shape[1], train_images.shape[2], train_images.shape[3]), cmap=plt.get_cmap('gray'))\n",
        "plt.xlabel(str(X_pca.shape[1])+'components', fontsize = 14)\n",
        "plt.title('image after pca', fontsize = 14)\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpv3Yfc20kIJ"
      },
      "source": [
        "## **Determine Number of Cluster**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soxkwwER0p6X"
      },
      "outputs": [],
      "source": [
        "visualizer = KElbowVisualizer(KMeans(random_state=10, max_iter = 1000), k=(1,11))\n",
        "\n",
        "visualizer.fit(X_pca)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1wiVweJ0sgB"
      },
      "source": [
        "## **K-Means**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDuzOUmA0rtG"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=3, n_jobs=-1, random_state=21, max_iter = 1000)\n",
        "kmeans.fit(X_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDRZkYqY0zxl"
      },
      "outputs": [],
      "source": [
        "y_kmeans = kmeans.fit_predict(X_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYvM_Ou401eP"
      },
      "source": [
        "## **K-Means Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k5h-Kt304ow"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9, 7))\n",
        "plt.scatter(X_pca[y_kmeans==0, 0], X_pca[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
        "plt.scatter(X_pca[y_kmeans==1, 0], X_pca[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
        "plt.scatter(X_pca[y_kmeans==2, 0], X_pca[y_kmeans==2, 1], s=100, c='green', label ='Cluster 3')\n",
        "# plt.scatter(X_pca[y_kmeans==3, 0], X_pca[y_kmeans==3, 1], s=100, c='cyan', label ='Cluster 4')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
        "plt.legend(loc = 4)\n",
        "plt.grid(None)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9WCXsR406kT"
      },
      "source": [
        "## **Cluster Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwm4bytr09MG"
      },
      "outputs": [],
      "source": [
        "# Jumlah data pada setiap cluster\n",
        "clustering_label = pd.DataFrame(y_kmeans)\n",
        "clustering_label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JanVqCDS0_YA"
      },
      "outputs": [],
      "source": [
        "# Melihat data hasil cluster\n",
        "Z = kmeans.predict(X_pca)\n",
        "\n",
        "for i in range(0,3):\n",
        "    image_label = []\n",
        "    row = np.where(Z==i)[0]\n",
        "    num = row.shape[0]          \n",
        "    r = int(np.floor(num/10.))    \n",
        "    \n",
        "    print(\"CLUSTER \"+str(i))\n",
        "    print(str(num)+\" elements\")\n",
        "\n",
        "    plt.figure(figsize=(12,10))\n",
        "    for k in range(0, num):\n",
        "        plt.subplot(r+1, 10, k+1)\n",
        "        image = train_images[row[k], ]\n",
        "        image = image.reshape(360, 640, 3)\n",
        "        image_label.append(train_labels[row[k], ])\n",
        "        plt.imshow(image, cmap=plt.cm.binary)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    print('Frequency of Each Category:')\n",
        "    print(pd.Series(image_label).value_counts())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmQ67PwUuS_H"
      },
      "source": [
        "# **Analisis Klasifikasi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N_ZVwmjvIbM"
      },
      "source": [
        "## **Metode Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku0UhOABuYmi"
      },
      "source": [
        "### **Fitur Ekstraksi Tekstur HOG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj6Sv5zM1ULD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from tensorflow import keras\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import itertools\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import shutil\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tenon9U1Vkr"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('youtube_link_detail.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBD17MeZudTx"
      },
      "source": [
        "**Fitur Ekstraksi Warna LAB-RGB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RebJtP103n_t"
      },
      "outputs": [],
      "source": [
        "labels = ['Rendah','Sedang','Tinggi']\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "image_size = 400 \n",
        "\n",
        "# All Data\n",
        "for i in labels:\n",
        "    folderPath = os.path.join(r\"E:\\00 KULYAH\\SEMESTER 5\\Data Mining II\\UAS\\Projek UAS\\train\",i)\n",
        "    for j in tqdm(os.listdir(folderPath)):\n",
        "        img = cv2.imread(os.path.join(folderPath,j))\n",
        "        try:\n",
        "            img = cv2.resize(img,(image_size,image_size))\n",
        "        except:\n",
        "            print(i, j)\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "        x_train.append(img)\n",
        "        y_train.append(i)\n",
        "print('Data train DONE!')\n",
        "\n",
        "# All Data\n",
        "for i in labels:\n",
        "    folderPath = os.path.join(r\"E:\\00 KULYAH\\SEMESTER 5\\Data Mining II\\UAS\\Projek UAS\\val\",i)\n",
        "    for j in tqdm(os.listdir(folderPath)):\n",
        "        img = cv2.imread(os.path.join(folderPath,j))\n",
        "        try:\n",
        "            img = cv2.resize(img,(image_size,image_size))\n",
        "        except:\n",
        "            print(i, j)\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "        x_test.append(img)\n",
        "        y_test.append(i)\n",
        "print('Data train DONE!')\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZMEVdTV6g4v"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D74AFaD3t9L"
      },
      "outputs": [],
      "source": [
        "x_train.shape\n",
        "# y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjNDRgcn6jy1"
      },
      "outputs": [],
      "source": [
        "df['Link'].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmfban__6l4u"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SEtVicS6nHv"
      },
      "outputs": [],
      "source": [
        "class_names = ['Rendah', 'Sedang', 'Tinggi']\n",
        "batas_bin = [100, 24999, 249999, 19000000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oEhBKcN6n3J"
      },
      "outputs": [],
      "source": [
        "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
        "print(class_names_label)\n",
        "nb_classes = len(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9WlKjBg6qmT"
      },
      "source": [
        "**Normalisasi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "844dNczr3uZb"
      },
      "outputs": [],
      "source": [
        "images_train_norm = []\n",
        "images_test_norm = []\n",
        "\n",
        "for i in x_train : \n",
        "    norm_train = i/255.0\n",
        "    images_train_norm.append(norm_train)\n",
        "\n",
        "for i in x_test: \n",
        "    norm_test = i/255.0\n",
        "    images_test_norm.append(norm_test)\n",
        "\n",
        "images_train_norm[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRQ7rPKv6u23"
      },
      "source": [
        "**HOG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF2rCFKP3zz7"
      },
      "outputs": [],
      "source": [
        "from skimage.feature import hog\n",
        "x_train_hog = []\n",
        "for i in range(len(x_train)):\n",
        "    fd , hog_im = hog(x_train[i] , orientations=9 , pixels_per_cell = (8,8),\n",
        "                     cells_per_block = (2,2) , visualize = True ,  multichannel = True)\n",
        "    x_train_hog.append(fd)\n",
        "\n",
        "x_train_hog = np.array(x_train_hog)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3Aa68h232GQ"
      },
      "outputs": [],
      "source": [
        "x_test_hog = []\n",
        "for i in range(len(x_test)):\n",
        "    fd , hog_im = hog(x_test[i] , orientations=9 , pixels_per_cell = (8,8),\n",
        "                     cells_per_block = (2,2) , visualize = True ,  multichannel = True)\n",
        "    x_test_hog.append(fd)\n",
        "\n",
        "x_test_hog = np.array(x_test_hog)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAfQfwJp35EQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def random_image_to_hog():\n",
        "    rnd = random.randint(0, 267)\n",
        "    img = x_train[rnd]\n",
        "    label = y_train\n",
        "    classes = ['Rendah', 'Sedang', 'Tinggi']\n",
        "    \n",
        "    fd, hog_img = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
        "                      cells_per_block=(2, 2), visualize=True, multichannel = True)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15), sharex=True, sharey=True)\n",
        "\n",
        "    ax1.axis('off')\n",
        "    ax1.imshow(img, cmap=plt.cm.gray)\n",
        "    ax1.set_title(classes)\n",
        "        \n",
        "    ax2.axis('off')\n",
        "    ax2.imshow(hog_img, cmap=plt.cm.gray)\n",
        "    ax2.set_title('HOG image')\n",
        "    plt.show()\n",
        "\n",
        "random_image_to_hog()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFVixm8vCyR"
      },
      "source": [
        "### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw6FxdeD641_"
      },
      "outputs": [],
      "source": [
        "# Hasil Perbandingan Performance Measure\n",
        "model_performance = pd.DataFrame(columns=['Accuracy','Sensitivity', 'Specificity', 'Precision','F1-Score'])\n",
        "# Hasil Perbandingan Accuracy Data Testing dan Data Training\n",
        "model_performance_tr = pd.DataFrame(columns=['Accuracy Training', 'Accuracy Testing'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cOQBpdq39Ti"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.reshape(-1,1)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OoN_t1_66yT"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.astype('uint8')\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4SAu0jx67TK"
      },
      "outputs": [],
      "source": [
        "len(x_train_hog)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZQqanXNvVo6"
      },
      "source": [
        "**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHI8mvj56-cs"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "from sklearn import svm\n",
        "model = svm.SVC(kernel = 'rbf')\n",
        "model.fit(x_train_hog , y_train.flatten())\n",
        "y_pred_svm= model.predict(x_test_hog)\n",
        "y_pred_tr= model.predict(x_train_hog)\n",
        "\n",
        "# Performance Measure\n",
        "print(\"\\n => Classification Report for Support Vector Machine:\\n%s\"%classification_report(y_test.flatten(), y_pred_svm))\n",
        "cm_svm=confusion_matrix(y_test.flatten(), y_pred_svm)\n",
        "sns.heatmap(cm_svm,cmap='BuPu',annot=True,fmt='d')\n",
        "print(\"\\n => Confusion Matrix for Support Vector Machine:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w48meIyJvYGc"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Ld2PnY7AL7"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc=RandomForestClassifier(random_state=0).fit(x_train_hog , y_train.flatten())\n",
        "y_pred_rfc= rfc.predict(x_test_hog)\n",
        "y_pred_tr= rfc.predict(x_train_hog)\n",
        "\n",
        "# Performance Measure\n",
        "print(\"\\n => Classification Report for Random Forest:\\n%s\"%classification_report(y_test.flatten(), y_pred_rfc))\n",
        "cm_svm=confusion_matrix(y_test.flatten(), y_pred_rfc)\n",
        "sns.heatmap(cm_svm,cmap='BuPu',annot=True,fmt='d')\n",
        "print(\"\\n => Confusion Matrix for Random Forest:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvYLD3NvveqF"
      },
      "source": [
        "### **Fitur Ekstraksi Tekstur GLCM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZ9Db8On1ekD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from tensorflow import keras\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import itertools\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import skimage\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S3t_sIj1gHx"
      },
      "source": [
        "**Connect to GDrive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99VBJook1jXd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfZOb8u5vhsm"
      },
      "source": [
        "**Fitur Ekstraksi Warna LAB-RGB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzDGtX-s1ky7"
      },
      "outputs": [],
      "source": [
        "labels = ['Rendah','Sedang','Tinggi']\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "image_size = 400 \n",
        "\n",
        "# All Data\n",
        "for i in labels:\n",
        "    folderPath = os.path.join('/content/drive/MyDrive/PROJECT UAS/train/',i)\n",
        "    for j in tqdm(os.listdir(folderPath)):\n",
        "        img = cv2.imread(os.path.join(folderPath,j))\n",
        "        try:\n",
        "            img = cv2.resize(img,(image_size,image_size))\n",
        "        except:\n",
        "            print(i, j)\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "        x_train.append(img)\n",
        "        y_train.append(i)\n",
        "print('Data train DONE!')\n",
        "\n",
        "# All Data\n",
        "for i in labels:\n",
        "    folderPath = os.path.join('/content/drive/MyDrive/PROJECT UAS/val/',i)\n",
        "    for j in tqdm(os.listdir(folderPath)):\n",
        "        img = cv2.imread(os.path.join(folderPath,j))\n",
        "        try:\n",
        "            img = cv2.resize(img,(image_size,image_size))\n",
        "        except:\n",
        "            print(i, j)\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "        x_test.append(img)\n",
        "        y_test.append(i)\n",
        "print('Data train DONE!')\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysbfWxizvi2E"
      },
      "source": [
        "**Fitur Ekstraksi Warna LAB-RGB-HSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O1QDl401nSo"
      },
      "outputs": [],
      "source": [
        "labels = ['Rendah','Sedang','Tinggi']\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "image_size = 400 \n",
        "\n",
        "# All Data\n",
        "for i in labels:\n",
        "    folderPath = os.path.join('/content/drive/MyDrive/PROJECT UAS/train/',i)\n",
        "    for j in tqdm(os.listdir(folderPath)):\n",
        "        img = cv2.imread(os.path.join(folderPath,j))\n",
        "        try:\n",
        "            img = cv2.resize(img,(image_size,image_size))\n",
        "        except:\n",
        "            print(i, j)\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "        x_train.append(img)\n",
        "        y_train.append(i)\n",
        "print('Data train DONE!')\n",
        "\n",
        "# All Data\n",
        "for i in labels:\n",
        "    folderPath = os.path.join('/content/drive/MyDrive/PROJECT UAS/val/',i)\n",
        "    for j in tqdm(os.listdir(folderPath)):\n",
        "        img = cv2.imread(os.path.join(folderPath,j))\n",
        "        try:\n",
        "            img = cv2.resize(img,(image_size,image_size))\n",
        "        except:\n",
        "            print(i, j)\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "        x_test.append(img)\n",
        "        y_test.append(i)\n",
        "print('Data train DONE!')\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyh_vVmh1whK"
      },
      "source": [
        "**GLCM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N9xHcH-10Ld"
      },
      "outputs": [],
      "source": [
        "from skimage.feature import greycomatrix, greycoprops\n",
        "\n",
        "# ----------------- calculate greycomatrix() & greycoprops() for angle 0, 45, 90, 135 ----------------------------------\n",
        "def calc_glcm_all_agls(img, label, props, dists=[5], agls=[0, np.pi/4, np.pi/2, 3*np.pi/4], lvl=256, sym=True, norm=True):\n",
        "    \n",
        "    # menghitung matrix GLCM dan matrix texture\n",
        "    glcm = greycomatrix(img, \n",
        "                        distances=dists, \n",
        "                        angles=agls, \n",
        "                        levels=lvl,\n",
        "                        symmetric=sym, \n",
        "                        normed=norm)\n",
        "    feature = []\n",
        "    glcm_props = [propery for name in props for propery in greycoprops(glcm, name)[0]]\n",
        "    for item in glcm_props:\n",
        "            feature.append(item)\n",
        "    feature.append(label) \n",
        "    \n",
        "    return feature\n",
        "\n",
        "\n",
        "# ----------------- call calc_glcm_all_agls() for all texture properties ----------------------------------\n",
        "properties = ['dissimilarity', 'correlation', 'homogeneity', 'contrast', 'ASM', 'energy']\n",
        "\n",
        "glcm_all_agls_xtrain = []\n",
        "for img, label in zip(img_train, y_train): \n",
        "    glcm_all_agls_xtrain.append(\n",
        "            calc_glcm_all_agls(img, \n",
        "                               label, \n",
        "                               props=properties)\n",
        "                            )\n",
        " \n",
        "columns = []\n",
        "angles = ['0', '45', '90','135']\n",
        "for name in properties :\n",
        "    for ang in angles:\n",
        "        columns.append(name + \"_\" + ang)\n",
        "        \n",
        "columns.append(\"label\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItY5ObWP1174"
      },
      "source": [
        "**Data Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arJyyv9i132J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "# Create the pandas DataFrame for GLCM features data\n",
        "glcm_train = pd.DataFrame(glcm_all_agls_xtrain, \n",
        "                             columns = columns)\n",
        "\n",
        "glcm_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUX-r7o216L2"
      },
      "outputs": [],
      "source": [
        "# Mengubah menjadi encoding\n",
        "glcm_train.loc[glcm_train[\"label\"] == \"Rendah\", \"label\"] = 0\n",
        "glcm_train.loc[glcm_train[\"label\"] == \"Sedang\", \"label\"] = 1\n",
        "glcm_train.loc[glcm_train[\"label\"] == \"Tinggi\", \"label\"] = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jymv9RH11633"
      },
      "source": [
        "**Data Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_XsJZRG182M"
      },
      "outputs": [],
      "source": [
        "img_test = []\n",
        "\n",
        "for img in x_test:\n",
        "    # convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    imgs = gray/255\n",
        "    imgs = imgs.astype(np.uint8)\n",
        "    # h, w = gray.shape\n",
        "    # ymin, ymax, xmin, xmax = h//3, h*2//3, w//3, w*2//3\n",
        "    # crop ROI\n",
        "    # crop = gray[ymin:ymax, xmin:xmax]\n",
        "    # resize = cv2.resize(crop, (0,0), fx=0.5, fy=0.5)\n",
        "    # save to list\n",
        "    img_test.append(imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4BefApO1-Yu"
      },
      "outputs": [],
      "source": [
        "from skimage.feature import greycomatrix, greycoprops\n",
        "\n",
        "# ----------------- calculate greycomatrix() & greycoprops() for angle 0, 45, 90, 135 ----------------------------------\n",
        "def calc_glcm_all_agls(img, label, props, dists=[5], agls=[0, np.pi/4, np.pi/2, 3*np.pi/4], lvl=256, sym=True, norm=True):\n",
        "    \n",
        "    # menghitung matrix GLCM dan matrix texture\n",
        "    glcm = greycomatrix(imgs, \n",
        "                        distances=dists, \n",
        "                        angles=agls, \n",
        "                        levels=lvl,\n",
        "                        symmetric=sym, \n",
        "                        normed=norm)\n",
        "    feature = []\n",
        "    glcm_props = [propery for name in props for propery in greycoprops(glcm, name)[0]]\n",
        "    for item in glcm_props:\n",
        "            feature.append(item)\n",
        "    feature.append(label) \n",
        "    \n",
        "    return feature\n",
        "\n",
        "\n",
        "# ----------------- call calc_glcm_all_agls() for all texture properties ----------------------------------\n",
        "properties = ['dissimilarity', 'correlation', 'homogeneity', 'contrast', 'ASM', 'energy']\n",
        "\n",
        "glcm_all_agls_xtest = []\n",
        "for img, label in zip(img_test, y_test): \n",
        "    glcm_all_agls_xtest.append(\n",
        "            calc_glcm_all_agls(img, \n",
        "                               label, \n",
        "                               props=properties)\n",
        "                            )\n",
        " \n",
        "columns = []\n",
        "angles = ['0', '45', '90','135']\n",
        "for name in properties :\n",
        "    for ang in angles:\n",
        "        columns.append(name + \"_\" + ang)\n",
        "        \n",
        "columns.append(\"label\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGebGQYR2A9b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "# Create the pandas DataFrame for GLCM features data\n",
        "glcm_test = pd.DataFrame(glcm_all_agls_xtest, \n",
        "                             columns = columns)\n",
        "\n",
        "glcm_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AC1CEBA2CbH"
      },
      "outputs": [],
      "source": [
        "# Mengubah menjadi encoding\n",
        "glcm_test.loc[glcm_test[\"label\"] == \"Rendah\", \"label\"] = 0\n",
        "glcm_test.loc[glcm_test[\"label\"] == \"Sedang\", \"label\"] = 1\n",
        "glcm_test.loc[glcm_test[\"label\"] == \"Tinggi\", \"label\"] = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxikhqX72DT4"
      },
      "source": [
        "**Mengubah Tipe Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACyOUMd02G6w"
      },
      "outputs": [],
      "source": [
        "xg_train = glcm_train.drop(columns = ['label']).astype('float32').values\n",
        "yg_train = glcm_train['label'].astype('float32').values\n",
        "xg_test = glcm_test.drop(columns = ['label']).astype('float32').values\n",
        "yg_test = glcm_test['label'].astype('float32').values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wbDAIoAvlSL"
      },
      "source": [
        "### **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jvr6YeovoIj"
      },
      "source": [
        "**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ima01Yo2JN-"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC \n",
        "svm_glcm = SVC(random_state = 77)\n",
        "svm_glcm.fit(xg_train, yg_train)\n",
        "\n",
        "pred_svm_glcm = svm_glcm.predict(xg_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "akurasi_svm_glcm = round(accuracy_score(yg_test, pred_svm_glcm),2)\n",
        "precision_svm_glcm = round(precision_score(yg_test, pred_svm_glcm, average='weighted'),2)\n",
        "recall_svm_glcm = round(recall_score(yg_test, pred_svm_glcm, average='weighted'),2)\n",
        "f1score_svm_glcm = round(f1_score(yg_test, pred_svm_glcm, average='weighted'),2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLv3TM4gvrAp"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RG77qLc2LMi"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_glcm = LogisticRegression(random_state = 77)\n",
        "lr_glcm.fit(xg_train, yg_train)\n",
        "\n",
        "pred_lr_glcm = lr_glcm.predict(xg_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "akurasi_lr_glcm = round(accuracy_score(yg_test, pred_lr_glcm),2)\n",
        "precision_lr_glcm = round(precision_score(yg_test, pred_lr_glcm, average='weighted'),2)\n",
        "recall_lr_glcm = round(recall_score(yg_test, pred_lr_glcm, average='weighted'),2)\n",
        "f1score_lr_glcm = round(f1_score(yg_test, pred_lr_glcm, average='weighted'),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuP6RVVHvts4"
      },
      "source": [
        "**XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDjcKJb12Muq"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "xgb_glcm = xgb.XGBClassifier(random_state = 77)\n",
        "xgb_glcm.fit(xg_train, yg_train)\n",
        "\n",
        "pred_xgb_glcm = xgb_glcm.predict(xg_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "akurasi_xgb_glcm = round(accuracy_score(yg_test, pred_xgb_glcm),2)\n",
        "precision_xgb_glcm = round(precision_score(yg_test, pred_xgb_glcm, average='weighted'),2)\n",
        "recall_xgb_glcm = round(recall_score(yg_test, pred_xgb_glcm, average='weighted'),2)\n",
        "f1score_xgb_glcm = round(f1_score(yg_test, pred_xgb_glcm, average='weighted'),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NdxtNu52Nrq"
      },
      "source": [
        "**Komparasi Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGigLR9Y2QMr"
      },
      "outputs": [],
      "source": [
        "compare_model1 = pd.DataFrame({'Model':['GLCM - XGBoost', 'GLCM - Logistic Regression', 'GLCM - Support Vector Machine'],\n",
        "                           'Accuracy':[akurasi_xgb_glcm, akurasi_lr_glcm, akurasi_svm_glcm],\n",
        "                           'Recall':[recall_xgb_glcm, recall_lr_glcm, recall_svm_glcm],\n",
        "                           'Precision':[precision_xgb_glcm, precision_lr_glcm, precision_svm_glcm],\n",
        "                           'F1-Score':[f1score_xgb_glcm, f1score_lr_glcm, f1score_svm_glcm]})\n",
        "\n",
        "compare_model1 = compare_model1.sort_values(by=['Accuracy', 'Precision', 'Recall', 'F1-Score'],ascending = [False, False, False, False])\n",
        "compare_model1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TesaNjevvxU"
      },
      "source": [
        "## **Metode Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uetr9_Mqv1Na"
      },
      "source": [
        "### **Klasik CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmbsnHhI3CdV"
      },
      "source": [
        "### **Arsitektur 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji4qHiXA2sQx"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (360, 640, 3)), \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY4uEpFx2ssP"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9_V8pB82uG-"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_images, train_labels, batch_size=16, epochs=20, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeFBYBvV2xAJ"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy_loss(history):\n",
        "    \"\"\"\n",
        "        Plot the accuracy and the loss during the training of the nn.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(221)\n",
        "    plt.plot(history.history['acc'],'bo--', label = \"acc\")\n",
        "    plt.plot(history.history['val_acc'], 'ro--', label = \"val_acc\")\n",
        "    plt.title(\"train_acc vs val_acc\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss function\n",
        "    plt.subplot(222)\n",
        "    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n",
        "    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n",
        "    plt.title(\"train_loss vs val_loss\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NDA3uVg2xot"
      },
      "outputs": [],
      "source": [
        "plot_accuracy_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFuuEYTB2zPn"
      },
      "outputs": [],
      "source": [
        "test_loss = model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzavrPoT3H8q"
      },
      "source": [
        "### **Arsitektur 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-6RHZ383LUq"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "# import tensorflow_addons as tfa\n",
        "# from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras import Sequential, Input\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Dropout,SeparableConv2D, Activation, BatchNormalization, Flatten, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GzlSNAp3L7a"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    shape_img = (360,640,3)\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(3))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWUxzmWn3NiM"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "hists = []\n",
        "\n",
        "# divisor = 5\n",
        "\n",
        "start_time = time.time()\n",
        "# X_train, y_train = load_img(cut_df(df,divisor,1))\n",
        "# y_train = to_categorical(y_train)\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
        "            ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "model.fit(train_images, train_labels, batch_size=128, epochs=10, callbacks=callbacks, validation_split = 0.1, verbose = 1)\n",
        "hists.append(model.history.history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fZLC8bp3PBR"
      },
      "outputs": [],
      "source": [
        "acc = []\n",
        "val_acc = []\n",
        "for i in range(len(hists)):\n",
        "    acc += hists[i][\"acc\"]\n",
        "    val_acc += hists[i][\"val_acc\"]\n",
        "hist_df = pd.DataFrame({\"# Epoch\": [e for e in range(1,len(acc)+1)],\"Accuracy\": acc, \"Val_accuracy\": val_acc})\n",
        "hist_df.plot(x = \"# Epoch\", y = [\"Accuracy\",\"Val_accuracy\"])\n",
        "plt.title(\"Accuracy vs Validation Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ2R2Q_qv4fV"
      },
      "source": [
        "\n",
        "### **Resnet50**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiHGKm4D23Et"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model,load_model, Sequential\n",
        "from tensorflow.keras.layers import  GlobalAveragePooling2D, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import  Adam\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UaL3Fz9244a"
      },
      "outputs": [],
      "source": [
        "base_model = ResNet50(weights= 'imagenet', include_top=False, input_shape= (360, 640, 3))\n",
        "resnet = base_model.output\n",
        "resnet = GlobalAveragePooling2D()(resnet)\n",
        "resnet = Dropout(0.25)(resnet)\n",
        "predictions = Dense(3, activation= 'softmax')(resnet)\n",
        "Resnet50 = Model(inputs = base_model.input, outputs = predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhNv6jxY26wF"
      },
      "outputs": [],
      "source": [
        "adam = Adam(learning_rate = 0.001)\n",
        "Resnet50.compile(optimizer= adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtCpoZUX28p_"
      },
      "outputs": [],
      "source": [
        "History = Resnet50.fit(train_images,\n",
        "                        train_labels, \n",
        "                        batch_size = 4,\n",
        "                        epochs = 2\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXvcEflV2_mu"
      },
      "outputs": [],
      "source": [
        "test_loss2 = Resnet50.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlvYsjKDwAy5"
      },
      "source": [
        "# **Menghitung Visual Attributes Thumbnail**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNYc9xikwSVK"
      },
      "source": [
        "## **Brightness**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTvlyt185lAY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import cv2\n",
        "import os\n",
        "import PIL.Image\n",
        "import imquality.brisque as brisque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL61muEd5hYA"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import math\n",
        "import glob\n",
        "\n",
        "image_list = []\n",
        "brightnes_list = []\n",
        "# Open the image file\n",
        "# for filename in glob.glob('E:/00 KULYAH/SEMESTER 5/Data Mining II/UAS/Projek UAS/all/*.jpg'):\n",
        "for filename in path:\n",
        "  im = Image.open(filename)\n",
        "  image_list.append(im)\n",
        "# Calculate the brightness using the formula:\n",
        "# brightness = sqrt(0.241 * R^2 + 0.691 * G^2 + 0.068 * B^2)\n",
        "# where R, G, and B are the red, green, and blue channels of the image\n",
        "  width, height = im.size\n",
        "  r_total = 0\n",
        "  g_total = 0\n",
        "  b_total = 0\n",
        "  for x in range(width):\n",
        "      for y in range(height):\n",
        "          r, g, b = im.getpixel((x, y))\n",
        "          r_total += r**2\n",
        "          g_total += g**2\n",
        "          b_total += b**2\n",
        "  brightness = math.sqrt(0.241 * r_total + 0.691 * g_total + 0.068 * b_total)\n",
        "  brightnes_list.append(brightness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH8seSVg5qDv"
      },
      "outputs": [],
      "source": [
        "data_complete = data_complete.assign(brightness=brightnes_list)\n",
        "data_complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "354QaNpcwPA8"
      },
      "source": [
        "## **Complexity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXebCG-O5rh_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "image_list = []\n",
        "complexity_list = []\n",
        "# Open the image file\n",
        "for filename in path :\n",
        "  im = Image.open(filename)\n",
        "  image = cv2.imread(filename)\n",
        "  image_list.append(im)\n",
        "# Load gambar dan konversi ke skala abu-abu\n",
        "# image = cv2.imread('WhatsApp Image 2022-12-03 at 00.32.45.jpg')\n",
        "  gambar_abu_abu = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Gunakan algoritma deteksi tepi Canny untuk menemukan tepi dalam gambar\n",
        "  tepi = cv2.Canny(gambar_abu_abu, 100, 200)\n",
        "  tepi = tepi.reshape(-1,1)\n",
        "\n",
        "# Gunakan Random Forest classifier untuk mengklasifikasi tepi sebagai sederhana atau kompleks\n",
        "  classifier = RandomForestClassifier()\n",
        "  classifier.fit(tepi, [0 if edge > 0 else 1 for edge in tepi])\n",
        "\n",
        "# Hitung skor kompleksitas komponen dengan mencari rata-rata prediksi classifier\n",
        "  skor_kompleksitas = classifier.predict(tepi).mean()\n",
        "  complexity_list.append(skor_kompleksitas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOKfqTq25t0_"
      },
      "outputs": [],
      "source": [
        "data_complete = data_complete.assign(complexity=complexity_list)\n",
        "data_complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twBk9vp9wVjX"
      },
      "source": [
        "## **Colorfulness**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekn0n76Z5xiB"
      },
      "outputs": [],
      "source": [
        "image_list = []\n",
        "colorfulness_list = []\n",
        "# Open the image file\n",
        "for filename in path:\n",
        "  im = Image.open(filename)\n",
        "  image = cv2.imread(filename)\n",
        "  image_list.append(im)\n",
        "\n",
        "# def image_colorfulness(image):\n",
        "\t# split the image into its respective RGB components\n",
        "  (B, G, R) = cv2.split(image.astype(\"float\"))\n",
        "\t# compute rg = R - G\n",
        "  rg = np.absolute(R - G)\n",
        "    # compute yb = 0.5 * (R + G) - B\n",
        "  yb = np.absolute(0.5 * (R + G) - B)\n",
        "  # compute the mean and standard deviation of both `rg` and `yb`\n",
        "  (rbMean, rbStd) = (np.mean(rg), np.std(rg))\n",
        "  (ybMean, ybStd) = (np.mean(yb), np.std(yb))\n",
        "  # combine the mean and standard deviations\n",
        "  stdRoot = np.sqrt((rbStd ** 2) + (ybStd ** 2))\n",
        "  meanRoot = np.sqrt((rbMean ** 2) + (ybMean ** 2))\n",
        "  # derive the \"colorfulness\" metric and return it\n",
        "  score = stdRoot + (0.3 * meanRoot)\n",
        "  colorfulness_list.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzSE8mHr5yNn"
      },
      "outputs": [],
      "source": [
        "data_complete = data_complete.assign(colorfulness=colorfulness_list)\n",
        "data_complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-kSeHY5wZRa"
      },
      "source": [
        "## **Quality**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzMc3bUp55Sh"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (640, 360)\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv2.imread(os.path.join(folder,filename))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, IMAGE_SIZE) \n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "            \n",
        "    return np.array(images, dtype='uint8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxPpY-TV554_"
      },
      "outputs": [],
      "source": [
        "def brisque_score(images):\n",
        "    score = []\n",
        "    for i in images:\n",
        "        score.append(brisque.score(i))\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySJVJpzM5633"
      },
      "outputs": [],
      "source": [
        "r = load_images_from_folder('train/Rendah/')\n",
        "s = load_images_from_folder('train/Sedang/')\n",
        "t = load_images_from_folder('train/Tinggi/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV5QZgbY58jP"
      },
      "outputs": [],
      "source": [
        "brisque_rendah = brisque_score(r)\n",
        "brisque_sedang = brisque_score(s)\n",
        "brisque_tinggi = brisque_score(t)\n",
        "re = os.listdir('train/Rendah/')\n",
        "se = os.listdir('train/Sedang/')\n",
        "ti = os.listdir('train/Tinggi/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zumGkxhM6Jf3"
      },
      "outputs": [],
      "source": [
        "df_r = pd.DataFrame({'Filename': re,\n",
        "                    'Label' : ['Rendah']*len(re),\n",
        "                    'BRISQUE': brisque_rendah})\n",
        "df_r.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqeHfUhf6L-N"
      },
      "outputs": [],
      "source": [
        "df_s = pd.DataFrame({'Filename': se,\n",
        "                    'Label' : ['Sedang']*len(se),\n",
        "                    'BRISQUE': brisque_sedang})\n",
        "df_s.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAzJTgc66NIx"
      },
      "outputs": [],
      "source": [
        "df_t = pd.DataFrame({'Filename': ti,\n",
        "                    'Label' : ['Tinggi']*len(ti),\n",
        "                    'BRISQUE': brisque_tinggi})\n",
        "df_t.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeLNF3kdwcMx"
      },
      "source": [
        "# **Analisis Regresi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQw9nGZU4c1G"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.stats.api as sms\n",
        "from scipy import stats\n",
        "from statsmodels.compat import lzip\n",
        "import statsmodels\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_VElkl-4d0h"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data_regresi.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnvTF0dy4fD5"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['Unnamed: 0', 'name','path'], axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XebeH-m4hlu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.lineplot(data=df, x=\"brightness\", y=\"Detail Views\", err_style=\"bars\",ci=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44DPnPu44iIY"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(data=df, x=\"complexity\", y=\"Detail Views\", err_style=\"bars\",ci=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCepubp24kTL"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(data=df, x=\"colorfulness\", y=\"Detail Views\", err_style=\"bars\",ci=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPdpEYsR4kte"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(data=df, x=\"quality\", y=\"Detail Views\", err_style=\"bars\",ci=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0F9J8Im4nNr"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXIsDS1b4o91"
      },
      "outputs": [],
      "source": [
        "# ====== Analisis Deskriptif ======\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDw-p0XU4pzA"
      },
      "outputs": [],
      "source": [
        "desc.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KChuqiGZ4t3w"
      },
      "source": [
        "**Visualisasi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2OSRQq44wXV"
      },
      "outputs": [],
      "source": [
        "#untuk mengetahui korelasi linier antar variabe\n",
        "mask = np.triu(np.ones_like(df.corr())) #agar yang muncul segitiga bawah\n",
        "annot = True #untuk mencetak/ menampilkan nilai korelasi di dalam kotak\n",
        "sns.set(rc={'figure.figsize':(13,6)}) \n",
        "sns.heatmap(df.corr(method='pearson').round(2), annot=True, cmap='OrRd', mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GR6Z6Il4yLz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,10)) \n",
        "sns.set_palette('Pastel1')\n",
        "plt.subplot(2,2,1)                                                                                                #bikin 2 baris, 3 kolom, kasi di indeks 1\n",
        "sns.regplot(x='brightness', y='Detail Views', data=df,line_kws={'color':'black'})\n",
        "plt.subplot(2,2,2)\n",
        "sns.regplot(x='complexity', y='Detail Views', data=df,line_kws={'color':'black'})\n",
        "plt.subplot(2,2,3)\n",
        "sns.regplot(x= 'colorfulness', y='Detail Views', data=df,line_kws={'color':'black'})\n",
        "plt.subplot(2,2,4)\n",
        "sns.regplot(x='quality', y='Detail Views', data=df,line_kws={'color':'black'})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXUN-AuS41PI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(13,13)) \n",
        "sns.set_palette('Spectral_r')\n",
        "plt.subplot(3,2,1)                                                                                                #bikin 2 baris, 3 kolom, kasi di indeks 1\n",
        "sns.distplot(df[ 'Detail Views'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.subplot(3,2,2)\n",
        "sns.distplot(df['brightness'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.subplot(3,2,3)\n",
        "sns.distplot(df['colorfulness'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.subplot(3,2,4)\n",
        "sns.distplot(df['complexity'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.subplot(3,2,5)\n",
        "sns.distplot(df[ 'quality'], bins=30, kde_kws={'linewidth': 5})\n",
        "# plt.subplot(3,2,6)\n",
        "# sns.distplot(df['price_share_usd'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7UCLQ_R4260"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB1o3WlF45Li"
      },
      "outputs": [],
      "source": [
        "def maximum_absolute_scaling(df):\n",
        "    # copy the dataframe\n",
        "    df_scaled = df.copy()\n",
        "    # apply maximum absolute scaling\n",
        "    for column in df_scaled.columns:\n",
        "        df_scaled[column] = df_scaled[column]  / df_scaled[column].abs().max()\n",
        "    return df_scaled\n",
        "    \n",
        "# call the maximum_absolute_scaling function\n",
        "df_cars_scaled = maximum_absolute_scaling(df_cars)\n",
        "\n",
        "df_cars_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FP8o1bb460b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "# create an abs_scaler object\n",
        "abs_scaler = MaxAbsScaler()\n",
        "\n",
        "# calculate the maximum absolute value for scaling the data using the fit method\n",
        "abs_scaler.fit(df_cars)\n",
        "\n",
        "# the maximum absolute values calculated by the fit method\n",
        "abs_scaler.max_abs_\n",
        "# array([4.0e+05, 1.7e+01])\n",
        "\n",
        "# transform the data using the parameters calculated by the fit method (the maximum absolute values)\n",
        "scaled_data = abs_scaler.transform(df_cars)\n",
        "\n",
        "# store the results in a data frame\n",
        "df_scaled = pd.DataFrame(scaled_data, columns=df_cars.columns)\n",
        "\n",
        "# visualize the data frame\n",
        "df_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqfFUbtp5ACq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# create a scaler object\n",
        "scaler = MinMaxScaler()\n",
        "# fit and transform the data\n",
        "df_norm = pd.DataFrame(scaler.fit_transform(df_cars), columns=df_cars.columns)\n",
        "\n",
        "df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRPdqjDN5Ahf"
      },
      "outputs": [],
      "source": [
        "# apply the z-score method in Pandas using the .mean() and .std() methods\n",
        "def z_score(df):\n",
        "    # copy the dataframe\n",
        "    df_std = df.copy()\n",
        "    # apply the z-score method\n",
        "    for column in df_std.columns:\n",
        "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
        "        \n",
        "    return df_std\n",
        "    \n",
        "# call the z_score function\n",
        "df_cars_standardized = z_score(df_cars)\n",
        "\n",
        "df_cars_standardized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRk35F3A5Dz2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "# create a scaler object\n",
        "scaler = RobustScaler()\n",
        "# fit and transform the data\n",
        "df_x = df[['brightness', 'complexity', 'colorfulness','quality']]\n",
        "df_robust = pd.DataFrame(scaler.fit_transform(df_x), columns=df_x.columns)\n",
        "df_robust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "picQ_gmY5Fbl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(13,13)) \n",
        "sns.set_palette('Spectral_r')\n",
        "plt.subplot(3,2,1)                                                                                                #bikin 2 baris, 3 kolom, kasi di indeks 1\n",
        "sns.distplot(df[ 'Detail Views'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.subplot(3,2,2)\n",
        "sns.distplot(df_robust['brightness'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.subplot(3,2,3)\n",
        "sns.distplot(df_robust['colorfulness'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.subplot(3,2,4)\n",
        "sns.distplot(df_robust['complexity'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.subplot(3,2,5)\n",
        "sns.distplot(df_robust[ 'quality'], bins=30, kde_kws={'linewidth': 5})\n",
        "# plt.subplot(3,2,6)\n",
        "# sns.distplot(df['price_share_usd'], bins=30, kde_kws={'linewidth': 5})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG4HY-vN5Gt5"
      },
      "outputs": [],
      "source": [
        "scaler = RobustScaler()\n",
        "# fit and transform the data\n",
        "df_x = df[['brightness', 'complexity', 'colorfulness','quality']]\n",
        "df[['brightness', 'complexity', 'colorfulness','quality']] = scaler.fit_transform(df_x)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cEM-8Uj5LC_"
      },
      "outputs": [],
      "source": [
        "df.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaY_EE5W5MV7"
      },
      "outputs": [],
      "source": [
        "t_sqrt = np.sqrt(df['Detail Views'])\n",
        "t_sqrt.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJc95O2q5M47"
      },
      "outputs": [],
      "source": [
        "t_ln = np.log(df['Detail Views'])\n",
        "t_ln.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_TmIype5PZs"
      },
      "outputs": [],
      "source": [
        "t_ln.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjlSCOt85QCc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "log10 = np.log10(df['Detail Views'])\n",
        "log10.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1ppYl9I5TOv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "t_1 = 1/(df['Detail Views'])\n",
        "t_1.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gXZuVe35UcR"
      },
      "outputs": [],
      "source": [
        "df['Detail Views']=np.log(df['Detail Views'])\n",
        "df['Detail Views'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xzH3COd5VzO"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(data=df, x=\"complexity\", y=\"Detail Views\", err_style=\"bars\",ci=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q8AOr9s5W1x"
      },
      "outputs": [],
      "source": [
        "# Analisis korelasi antar variabel dalam dataset\n",
        "mask = np.triu(np.ones_like(df.corr())) #agar yang muncul segitiga bawah\n",
        "annot = True #untuk mencetak/ menampilkan nilai korelasi di dalam kotak\n",
        "sns.set(rc={'figure.figsize':(30,13)}) \n",
        "sns.heatmap(df.corr(method='pearson').round(2), annot=True, cmap='viridis', mask=mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgoxPTi7wkjk"
      },
      "source": [
        "## **OLS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L75vz9YV5XhK"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "#summary\n",
        "x = df[['brightness','complexity','colorfulness','quality']]\n",
        "y = df['Detail Views']\n",
        "\n",
        "#summary\n",
        "X = sm.add_constant(x)\n",
        "result1 = sm.OLS(y, X).fit()\n",
        "print(result1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz-kYozt5Y80"
      },
      "outputs": [],
      "source": [
        "y_pred = result1.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfzmKNtP5bK_"
      },
      "outputs": [],
      "source": [
        "MSE = np.square(np.subtract(y,y_pred)).mean()\n",
        "MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4CpO1Kw5caO"
      },
      "outputs": [],
      "source": [
        "from patsy import dmatrices\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "# VIF dataframe\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
        "                          for i in range(len(X.columns))]\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8zFX97E5dbs"
      },
      "outputs": [],
      "source": [
        "#Mengambil nilai residual dalam summary\n",
        "residual = result1.resid\n",
        "#Membuat scatter plot residual vs time order\n",
        "#Membuat list dari time order dataset\n",
        "time1=list(range(1,334+1))\n",
        "print (time1)\n",
        "#Menampilkan sebaran data dalam scatter plot residual vs time order\n",
        "plt.scatter(x=time1, y=residual)\n",
        "plt.xlabel('time order', fontsize=12)\n",
        "plt.ylabel('residuals', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od9ERR3x5e63"
      },
      "outputs": [],
      "source": [
        "# membuat plot residuals vs fitted value\n",
        "plt.scatter(x=result1.fittedvalues, y=residual)\n",
        "plt.xlabel('fitted value', fontsize=12)\n",
        "plt.ylabel('residuals', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdiAH_Qy5gMt"
      },
      "outputs": [],
      "source": [
        "# Uji Kolmogorov Smirnov untuk uji normalitas data residual\n",
        "from scipy.stats import kstest\n",
        "ks1=kstest(residual,'norm')\n",
        "print('Statistic KS:', ks1.statistic.round(4))\n",
        "print('P-value:', ks1.pvalue.round(4))\n",
        "alpha = 0.05\n",
        "if ks1.pvalue > alpha:\n",
        "\tprint('Data Berdistribusi Normal (Gagal Tolak H0)')\n",
        "else:\n",
        "\tprint('Data Tidak Berdistribusi Normal (Tolak H0)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCYkFY3UwmZt"
      },
      "source": [
        "## **Random Forest Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMlKhfmM5g0C"
      },
      "outputs": [],
      "source": [
        "#summary\n",
        "X = df[['brightness','complexity','colorfulness','quality']]\n",
        "y = df['Detail Views']\n",
        "\n",
        "# Make necessary imports\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)\n",
        "\n",
        "# Train the model\n",
        "regr = RandomForestRegressor(n_estimators = 1100, max_depth = 70, random_state = 101)\n",
        "regr.fit(X_train, y_train.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRcZrs6g5iSr"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Make prediction\n",
        "predictions = regr.predict(X_test)\n",
        "\n",
        "result = X_test\n",
        "result['Detail Views'] = y_test\n",
        "result['Prediction'] = predictions.tolist()\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRUuq8xo5kMI"
      },
      "outputs": [],
      "source": [
        "# Import library for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "# Define x axis\n",
        "x_axis = X_test.brightness\n",
        "# Build scatterplot\n",
        "plt.scatter(x_axis, y_test, c = 'b', alpha = 1, marker = '.', label = 'Real')\n",
        "plt.scatter(x_axis, predictions, c = 'r', alpha = 1, marker = '.', label = 'Predicted')\n",
        "plt.xlabel('brightness')\n",
        "plt.ylabel('Price')\n",
        "plt.grid(color = '#D3D3D3', linestyle = 'solid')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUDnGWU65lin"
      },
      "outputs": [],
      "source": [
        "# Import library for metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Mean absolute error (MAE)\n",
        "mae = mean_absolute_error(y_test.values.ravel(), predictions)\n",
        "\n",
        "# Mean squared error (MSE)\n",
        "mse = mean_squared_error(y_test.values.ravel(), predictions)\n",
        "\n",
        "# R-squared scores\n",
        "r2 = r2_score(y_test.values.ravel(), predictions)\n",
        "\n",
        "# Print metrics\n",
        "print('Mean Absolute Error:', round(mae, 2))\n",
        "print('Mean Squared Error:', round(mse, 2))\n",
        "print('R-squared scores:', round(r2, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GALjkk955m6c"
      },
      "outputs": [],
      "source": [
        "# Import GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Find the best parameters for the model\n",
        "parameters = {\n",
        "    'max_depth': [70, 80, 90, 100],\n",
        "    'n_estimators': [900, 1000, 1100]\n",
        "}\n",
        "gridforest = GridSearchCV(regr, parameters, cv = 3, n_jobs = -1, verbose = 1)\n",
        "gridforest.fit(X_train, y_train)\n",
        "gridforest.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvUX7t8i5oPb"
      },
      "outputs": [],
      "source": [
        "# Get features list\n",
        "characteristics = X.columns\n",
        "# Get the variables importances, sort them, and print the result\n",
        "importances = list(regr.feature_importances_)\n",
        "characteristics_importances = [(characteristic, round(importance, 2)) for characteristic, importance in zip(characteristics, importances)]\n",
        "characteristics_importances = sorted(characteristics_importances, key = lambda x: x[1], reverse = True)\n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in characteristics_importances];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ_0A0Iu5p3e"
      },
      "outputs": [],
      "source": [
        "# Visualize the variables importances\n",
        "plt.bar(characteristics, importances, orientation = 'vertical')\n",
        "plt.xticks(rotation = 'vertical')\n",
        "plt.ylabel('Importance')\n",
        "plt.xlabel('Variable')\n",
        "plt.grid(axis = 'y', color = '#D3D3D3', linestyle = 'solid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZFXM7H5qgk"
      },
      "source": [
        "sc : https://www.datacareer.de/blog/random-forest-in-python-with-scikit-learn/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_k-6m_AwtCo"
      },
      "source": [
        "## **SVM Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BVgVLn05svW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i8xsJCF5uNm"
      },
      "outputs": [],
      "source": [
        "np.random.seed(21)\n",
        "N = 1000    \n",
        "def makeData(x):    \n",
        "    r = [a/10 for a in x]\n",
        "    y = np.sin(x)+np.random.uniform(-.5, .2, len(x))\n",
        "    return np.array(y+r)\n",
        "\n",
        "x = [i/100 for i in range(N)]\n",
        "y = makeData(x)\n",
        "x = np.array(x).reshape(-1,1)\n",
        "\n",
        "plt.scatter(x, y, s=5, color=\"blue\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OOCgNju5vb2"
      },
      "outputs": [],
      "source": [
        "#summary\n",
        "X = df[['brightness','complexity','colorfulness','quality']]\n",
        "y = df['Detail Views']\n",
        "\n",
        "svr = SVR().fit(X, y)\n",
        "print(svr)\n",
        "\n",
        "yfit = svr.predict(X)\n",
        "score = svr.score(X,y)\n",
        "print(\"R-squared:\", score)\n",
        "print(\"MSE:\", mean_squared_error(y, yfit))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtcKFGCrwvg5"
      },
      "source": [
        "## **ANN Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsFh80ih59D2"
      },
      "outputs": [],
      "source": [
        "# Separate Target Variable and Predictor Variables\n",
        "TargetVariable=['Detail Views']\n",
        "Predictors=['brightness','complexity','colorfulness','quality']\n",
        "\n",
        "X=df[Predictors].values\n",
        "y=df[TargetVariable].values\n",
        "### Sandardization of data ###\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "PredictorScaler=StandardScaler()\n",
        "TargetVarScaler=StandardScaler()\n",
        "# Storing the fit object for later reference\n",
        "PredictorScalerFit=PredictorScaler.fit(X)\n",
        "TargetVarScalerFit=TargetVarScaler.fit(y)\n",
        "# Generating the standardized values of X and y\n",
        "X=PredictorScalerFit.transform(X)\n",
        "y=TargetVarScalerFit.transform(y)\n",
        "# Split the data into training and testing set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Quick sanity check with the shapes of Training and testing datasets\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jla5upgP59uL"
      },
      "outputs": [],
      "source": [
        "# Installing required libraries\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6_gRY9mi6AE3"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25860/713476946.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# create ANN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Defining the Input layer and FIRST hidden layer, both are same!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# create ANN model\n",
        "model = Sequential()\n",
        "# Defining the Input layer and FIRST hidden layer, both are same!\n",
        "model.add(Dense(units=5, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
        "# Defining the Second layer of the model\n",
        "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
        "model.add(Dense(units=5, kernel_initializer='normal', activation='tanh'))\n",
        "# The output neuron is a single fully connected node \n",
        "# Since we will be predicting a single number\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "# Compiling the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "# Fitting the ANN to the Training set\n",
        "model.fit(X_train, y_train ,batch_size = 20, epochs = 50, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4GVvGbx6CRz"
      },
      "outputs": [],
      "source": [
        "# Defining a function to find the best parameters for ANN\n",
        "def FunctionFindBestParams(X_train, y_train, X_test, y_test):\n",
        "    \n",
        "    # Defining the list of hyper parameters to try\n",
        "    batch_size_list=[5, 10, 15, 20]\n",
        "    epoch_list  =   [5, 10, 50, 100]\n",
        "    \n",
        "    import pandas as pd\n",
        "    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])\n",
        "    \n",
        "    # initializing the trials\n",
        "    TrialNumber=0\n",
        "    for batch_size_trial in batch_size_list:\n",
        "        for epochs_trial in epoch_list:\n",
        "            TrialNumber+=1\n",
        "            # create ANN model\n",
        "            model = Sequential()\n",
        "            # Defining the first layer of the model\n",
        "            model.add(Dense(units=5, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "            # Defining the Second layer of the model\n",
        "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "            # The output neuron is a single fully connected node \n",
        "            # Since we will be predicting a single number\n",
        "            model.add(Dense(1, kernel_initializer='normal'))\n",
        "\n",
        "            # Compiling the model\n",
        "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "            # Fitting the ANN to the Training set\n",
        "            model.fit(X_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=0)\n",
        "\n",
        "            MAPE = np.mean(100 * (np.abs(y_test-model.predict(X_test))/y_test))\n",
        "            \n",
        "            # printing the results of the current iteration\n",
        "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', 100-MAPE)\n",
        "            \n",
        "            SearchResultsData=SearchResultsData.append(pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), 100-MAPE]],\n",
        "                                                                    columns=['TrialNumber', 'Parameters', 'Accuracy'] ))\n",
        "    return(SearchResultsData)\n",
        "\n",
        "\n",
        "######################################################\n",
        "# Calling the function\n",
        "ResultsData=FunctionFindBestParams(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QstEOuqO6D5_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "ResultsData.plot(x='Parameters', y='Accuracy', figsize=(15,4), kind='line')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHG6uqCU6E5r"
      },
      "outputs": [],
      "source": [
        "# Fitting the ANN to the Training set\n",
        "model.fit(X_train, y_train ,batch_size = 15, epochs = 5, verbose=0)\n",
        "\n",
        "# Generating Predictions on testing data\n",
        "Predictions=model.predict(X_test)\n",
        "\n",
        "# Scaling the predicted Price data back to original price scale\n",
        "Predictions=TargetVarScalerFit.inverse_transform(Predictions)\n",
        "\n",
        "# Scaling the y_test Price data back to original price scale\n",
        "y_test_orig=TargetVarScalerFit.inverse_transform(y_test)\n",
        "\n",
        "# Scaling the test data back to original scale\n",
        "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
        "\n",
        "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
        "TestingData['Detail Views']=y_test_orig\n",
        "TestingData['Predicted Detail Views']=Predictions\n",
        "TestingData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNmXHUdp6GLK"
      },
      "outputs": [],
      "source": [
        "# Computing the absolute percent error\n",
        "APE=100*(abs(TestingData['Detail Views']-TestingData['Predicted Detail Views'])/TestingData['Detail Views'])\n",
        "TestingData['APE']=APE\n",
        "\n",
        "print('The Accuracy of ANN model is:', 100-np.mean(APE))\n",
        "TestingData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKisPyjb6HO0"
      },
      "outputs": [],
      "source": [
        "# Function to generate Deep ANN model \n",
        "def make_regression_ann(Optimizer_trial):\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal'))\n",
        "    model.compile(loss='mean_squared_error', optimizer=Optimizer_trial)\n",
        "    return model\n",
        "\n",
        "###########################################\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "# Listing all the parameters to try\n",
        "Parameter_Trials={'batch_size':[10,20],\n",
        "                      'epochs':[10,20],\n",
        "                    'Optimizer_trial':['adam', 'rmsprop']\n",
        "                 }\n",
        "\n",
        "# Creating the regression ANN model\n",
        "RegModel=KerasRegressor(make_regression_ann, verbose=0)\n",
        "\n",
        "###########################################\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Defining a custom function to calculate accuracy\n",
        "def Accuracy_Score(orig,pred):\n",
        "    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))\n",
        "    print('#'*70,'Accuracy:', 100-MAPE)\n",
        "    return(100-MAPE)\n",
        "\n",
        "custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n",
        "\n",
        "#########################################\n",
        "# Creating the Grid search space\n",
        "# See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
        "grid_search=GridSearchCV(estimator=RegModel, \n",
        "                         param_grid=Parameter_Trials, \n",
        "                         scoring=custom_Scoring, \n",
        "                         cv=5)\n",
        "\n",
        "#########################################\n",
        "# Measuring how much time it took to find the best params\n",
        "import time\n",
        "StartTime=time.time()\n",
        "\n",
        "# Running Grid Search for different paramenters\n",
        "grid_search.fit(X,y, verbose=0)\n",
        "\n",
        "EndTime=time.time()\n",
        "print(\"########## Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')\n",
        "\n",
        "print('### Printing Best parameters ###')\n",
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGNX2uNV6R8l"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "  \n",
        "# Given values\n",
        "Y_true = TestingData['Detail Views']\n",
        "  \n",
        "# calculated values\n",
        "Y_pred = TestingData['Predicted Detail Views']\n",
        "  \n",
        "# Calculation of Mean Squared Error (MSE)\n",
        "print('Nilai MSE: ',mean_squared_error(Y_true,Y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad659c4510e8327c9668cc50a85cbb14a85502d291f4ab83c5d063e351f9ccf8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
